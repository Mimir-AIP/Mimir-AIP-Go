# Storage

Backend Orchestrater server/container will use an abstract, tabular storage interface and user can use different modular plugins to determine where and how their data is stored(this could be SQL, S3, Mongo, Supabase etc., neo4j etc.) Backend orchestrater should not need to 'care' about how the data is actually being stored the conversion from abstract to specifics for both storage and retrieval is handled by the plugin. The backend orchestrater will convert the project's ontology into a tabular schema which will be used to store data for that project. Plugins may override or ammend/convert this schema as needed to fit the specific storage type, but the backend orchestrater will always interact with the storage through the abstract tabular schema. When pipelines are executed and new data is ingested, this will be stored in the storage according to the structure defined by the ontology and the tabular schema. When ML models are being trained or making inferences, they will retrieve data from storage using the same structure. Finally when building the digital twin, it will also retrieve data from storage to populate the entities, attributes and relationships defined in the ontology.(Digital twins do not store a entire copy of the data rather they query and reference the data, and if data modification it will store only the changed values seperately and reference the original data for unchanged values)