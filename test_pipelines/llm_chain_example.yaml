# LLM Chain Example: Generate → Summarize → Refine
# This pipeline demonstrates a multi-step LLM chain:
# 1. Generate initial content
# 2. Summarize the generated content
# 3. Refine the summary into a final output

pipelines:
  - name: "LLM Chain Pipeline"
    description: "Chains multiple LLM calls to generate, summarize, and refine content"
    steps:
      - name: "Generate Initial Content"
        plugin: "AIModels.openai"
        config:
          operation: "chat"
          model: "gpt-3.5-turbo"
          messages:
            - role: "user"
              content: "Write a brief explanation of quantum computing for beginners."
          max_tokens: 200
          temperature: 0.8
          api_key: "${OPENAI_API_KEY}"
        output: "initial_content"

      - name: "Summarize Content"
        plugin: "AIModels.openai"
        config:
          operation: "chat"
          model: "gpt-3.5-turbo"
          messages:
            - role: "system"
              content: "You are a summarizer. Create a concise summary of the provided text in one sentence."
            - role: "user"
              content: "Please summarize: {{.initial_content.content}}"
          max_tokens: 80
          temperature: 0.3
          api_key: "${OPENAI_API_KEY}"
        output: "summary"

      - name: "Refine Summary"
        plugin: "AIModels.openai"
        config:
          operation: "chat"
          model: "gpt-3.5-turbo"
          messages:
            - role: "system"
              content: "You are an editor. Refine the provided summary to be clear, accurate, and engaging for a general audience."
            - role: "user"
              content: "Refine this summary: {{.summary.content}}"
          max_tokens: 100
          temperature: 0.5
          api_key: "${OPENAI_API_KEY}"
        output: "refined_summary"

      - name: "Save Final Output"
        plugin: "Output.json"
        config:
          file_path: "./output/llm_chain_result.json"
          input: "refined_summary"
          pretty: true
        output: "final_result"

# Usage:
# 1. Set OPENAI_API_KEY environment variable
# 2. Run: mimir-aip run test_pipelines/llm_chain_example.yaml
# 3. Check output in ./output/llm_chain_result.json